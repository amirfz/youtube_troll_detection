{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize, LabelEncoder\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('youtube-comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>troll</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a lucky guy, got celebrated birthday on c...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love it﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no americans even knew who corden was several ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my birthday was the 22nd as well and we both s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMG IM CRYING﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  troll  \\\n",
       "0  What a lucky guy, got celebrated birthday on c...      0   \n",
       "1                                           Love it﻿      0   \n",
       "2  no americans even knew who corden was several ...      0   \n",
       "3  my birthday was the 22nd as well and we both s...      0   \n",
       "4                                     OMG IM CRYING﻿      0   \n",
       "\n",
       "                                               title   views  dislikes  \\\n",
       "0  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "1  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "2  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "3  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "4  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "\n",
       "   commentCount  likes  replies           id  \n",
       "0          1123  19392        0  mak_Cu9Wl6w  \n",
       "1          1123  19392        0  mak_Cu9Wl6w  \n",
       "2          1123  19392        0  mak_Cu9Wl6w  \n",
       "3          1123  19392        0  mak_Cu9Wl6w  \n",
       "4          1123  19392        0  mak_Cu9Wl6w  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    '''Function to clean the text data and prep for further analysis'''\n",
    "    stops = set(stopwords.words(\"english\"))     # Creating a set of Stopwords\n",
    "    p_stemmer = PorterStemmer()                 # Creating the stemmer model\n",
    "    text = re.sub(\"&#39;\",'', text)\n",
    "    text = re.sub(r\"</?\\w+[^>]*>\", ' tag ', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    text = re.sub(\"[^a-zA-Z@!0-9]\", ' ', text)\n",
    "    text = text.split()                          # Splits the data into individual words \n",
    "    text = [w for w in text if not w in stops]   # Removes stopwords\n",
    "    text = [p_stemmer.stem(i) for i in text]     # Stemming (reducing words to their root)\n",
    "    if not len(text):                            # dealing with comments that are all emojis, stop words or other languages\n",
    "        text = ['emostwol']\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['clean'] = data['comment'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>troll</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>id</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a lucky guy, got celebrated birthday on c...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>What lucki guy got celebr birthday concert bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love it﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no americans even knew who corden was several ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>american even knew corden sever year ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my birthday was the 22nd as well and we both s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>birthday 22nd well support west ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMG IM CRYING﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>OMG IM CRi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  troll  \\\n",
       "0  What a lucky guy, got celebrated birthday on c...      0   \n",
       "1                                           Love it﻿      0   \n",
       "2  no americans even knew who corden was several ...      0   \n",
       "3  my birthday was the 22nd as well and we both s...      0   \n",
       "4                                     OMG IM CRYING﻿      0   \n",
       "\n",
       "                                               title   views  dislikes  \\\n",
       "0  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "1  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "2  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "3  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "4  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "\n",
       "   commentCount  likes  replies           id  \\\n",
       "0          1123  19392        0  mak_Cu9Wl6w   \n",
       "1          1123  19392        0  mak_Cu9Wl6w   \n",
       "2          1123  19392        0  mak_Cu9Wl6w   \n",
       "3          1123  19392        0  mak_Cu9Wl6w   \n",
       "4          1123  19392        0  mak_Cu9Wl6w   \n",
       "\n",
       "                                               clean  \n",
       "0  What lucki guy got celebr birthday concert bil...  \n",
       "1                                               Love  \n",
       "2           american even knew corden sever year ago  \n",
       "3                birthday 22nd well support west ham  \n",
       "4                                         OMG IM CRi  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BadWordCounter(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        with open(\"my_badlist3.txt\") as f:\n",
    "            badwords = [l.strip() for l in f.readlines()]\n",
    "        self.badwords_ = badwords\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return np.array(['n_words', 'n_chars', 'allcaps', 'max_len',\n",
    "            'mean_len', '@', '!', 'spaces', 'bad_ratio', 'n_bad',\n",
    "            'capsratio'])\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents): \n",
    "        ## some handcrafted features!\n",
    "        n_words = [len(c.split()) for c in documents]\n",
    "        n_chars = [len(c) for c in documents]\n",
    "        # number of uppercase words\n",
    "        allcaps = [np.sum([w.isupper() for w in c.split()])\n",
    "               for c in documents]\n",
    "        # longest word\n",
    "        max_word_len = [np.max([len(w) for w in c.split()]) for c in (documents)]\n",
    "        # average word length\n",
    "        mean_word_len = [np.mean([len(w) for w in c.split()])\n",
    "                                            for c in (documents)]\n",
    "        # number of google badwords:\n",
    "        n_bad = [np.sum([c.lower().count(w) for w in self.badwords_])\n",
    "                                                for c in documents]\n",
    "        exclamation = [c.count(\"!\") for c in documents]\n",
    "        addressing = [c.count(\"@\") for c in documents]\n",
    "        spaces = [c.count(\" \") for c in documents]\n",
    "\n",
    "        n_words[n_words==0] = 1\n",
    "        allcaps_ratio = np.array(allcaps) / np.array(n_words, dtype=np.float)\n",
    "        bad_ratio = np.array(n_bad) / np.array(n_words, dtype=np.float)\n",
    "        \n",
    "        output = np.array([n_words, n_chars, allcaps, max_word_len,\n",
    "            mean_word_len, exclamation, addressing, spaces, bad_ratio, n_bad,\n",
    "            allcaps_ratio]).T\n",
    "\n",
    "        return normalize(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureStacker(BaseEstimator):\n",
    "    \"\"\"Stacks several transformer objects to yield concatenated features.\n",
    "    Similar to pipeline, a list of tuples ``(name, estimator)`` is passed\n",
    "    to the constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer_list):\n",
    "        self.transformer_list = transformer_list\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for name, trans in self.transformer_list:\n",
    "            trans.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for name, trans in self.transformer_list:\n",
    "            features.append(trans.transform(X))\n",
    "        issparse = [sparse.issparse(f) for f in features]\n",
    "        if np.any(issparse):\n",
    "            features = sparse.hstack(features).tocsr()\n",
    "        else:\n",
    "            features = np.hstack(features)\n",
    "        return features\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        if not deep:\n",
    "            return super(FeatureStacker, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = dict(self.transformer_list)\n",
    "            for name, trans in self.transformer_list:\n",
    "                for key, value in trans.get_params(deep=True).iteritems():\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_word = TfidfVectorizer(lowercase=False,\n",
    "                             analyzer=u'word',\n",
    "                             ngram_range=(1, 3),\n",
    "                             stop_words='english',\n",
    "                             binary=False,\n",
    "                             norm=u'l2', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True, \n",
    "                             sublinear_tf=True,\n",
    "                             min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_char = TfidfVectorizer(lowercase=False,\n",
    "                             analyzer=u'char',\n",
    "                             ngram_range=(1, 5),\n",
    "                             stop_words='english',\n",
    "                             binary=False,\n",
    "                             norm=u'l2', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True, \n",
    "                             sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select = SelectPercentile(score_func=chi2, percentile=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=1e-8, penalty='l2', C=8, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "badwords = BadWordCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ft = FeatureStacker([(\"badwords\", badwords), (\"chars\", vectorizer_char), (\"words\", vectorizer_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', ft), ('select', select), ('logr', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(data.troll.values)\n",
    "Y = le.transform(data.troll.values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doing a grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is really slow on my machine, need a proper GPU to run this properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    #'vectorizer_char__ngram_range': ((1, 2), (1, 3), (1,5)),  # unigrams or bigrams\n",
    "    #'vectorizer_word__use_idf': (True, False),\n",
    "    #'vectorizer_word__norm': ('l1', 'l2'),\n",
    "    #'select__percentile': (0.1, 1, 10),\n",
    "    'clf__C': (1, 7)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, scoring=make_scorer(f1_score))\n",
    "grid_search.fit(data.clean, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.clean, Y, train_size=.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.fit(X_train,y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = confusion_matrix(y_test, pred)\n",
    "    \n",
    "print('confidence matrix:')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf_labels = model.predict(data.clean)\n",
    "print('True Positives:')\n",
    "print('================')\n",
    "print(data['comment'][(clf_labels==1) & (data.troll==1)])\n",
    "print('False Positives:')\n",
    "print('================')\n",
    "print(data['comment'][(clf_labels==1) & (data.troll==0)])\n",
    "print('False Negatives:')\n",
    "print('================')\n",
    "print(data['comment'][(clf_labels==0) & (data.troll==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.named_steps['select'].get_support().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions/ comments\n",
    "* does it make sense to do variance based feature selection when dealing with imbalanced label sizes?\n",
    "* tf-idf vectorizer gives around 5500 words (columns) and that's more than the number of rows, and since most words are redundant anyway it is important to reduce the dimensionality of the problem\n",
    "* the fact that only 10% of the data is trolls means that maybe an anamoly detection approach would perform better, this is taken care of by giving the classifier the flag class_weight='balanced' that ensures a balanced sampling of the input data\n",
    "* use reg experssions to remove obvious things like links, etc\n",
    "* do setiment analysis as an extra feature\n",
    "* LogReg seems to perform better than RandForest, esp given the mismatch in label size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
