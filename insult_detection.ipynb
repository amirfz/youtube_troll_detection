{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize, LabelEncoder\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('youtube-comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>troll</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a lucky guy, got celebrated birthday on c...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love itï»¿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no americans even knew who corden was several ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my birthday was the 22nd as well and we both s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMG IM CRYINGï»¿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  troll  \\\n",
       "0  What a lucky guy, got celebrated birthday on c...      0   \n",
       "1                                           Love itï»¿      0   \n",
       "2  no americans even knew who corden was several ...      0   \n",
       "3  my birthday was the 22nd as well and we both s...      0   \n",
       "4                                     OMG IM CRYINGï»¿      0   \n",
       "\n",
       "                                               title   views  dislikes  \\\n",
       "0  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "1  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "2  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "3  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "4  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "\n",
       "   commentCount  likes  replies           id  \n",
       "0          1123  19392        0  mak_Cu9Wl6w  \n",
       "1          1123  19392        0  mak_Cu9Wl6w  \n",
       "2          1123  19392        0  mak_Cu9Wl6w  \n",
       "3          1123  19392        0  mak_Cu9Wl6w  \n",
       "4          1123  19392        0  mak_Cu9Wl6w  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    '''Function to clean the text data and prep for further analysis'''\n",
    "    stops = set(stopwords.words(\"english\"))     # Creating a set of Stopwords\n",
    "    p_stemmer = PorterStemmer()                 # Creating the stemmer model\n",
    "    text = re.sub(\"&#39;\",'', text)\n",
    "    text = re.sub(r\"</?\\w+[^>]*>\", ' tag ', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    text = re.sub(\"[^a-zA-Z@!0-9]\", ' ', text)\n",
    "    text = text.split()                          # Splits the data into individual words \n",
    "    text = [w for w in text if not w in stops]   # Removes stopwords\n",
    "    text = [p_stemmer.stem(i) for i in text]     # Stemming (reducing words to their root)\n",
    "    if not len(text):                            # dealing with comments that are all emojis, stop words or other languages\n",
    "        text = ['emostwol']\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['clean'] = data['comment'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>troll</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>id</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a lucky guy, got celebrated birthday on c...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>What lucki guy, got celebr birthday concert bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love itï»¿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>Love itï»¿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no americans even knew who corden was several ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>american even knew corden sever year agoï»¿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my birthday was the 22nd as well and we both s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>birthday 22nd well support west hamï»¿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMG IM CRYINGï»¿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>OMG IM CRYINGï»¿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  troll  \\\n",
       "0  What a lucky guy, got celebrated birthday on c...      0   \n",
       "1                                           Love itï»¿      0   \n",
       "2  no americans even knew who corden was several ...      0   \n",
       "3  my birthday was the 22nd as well and we both s...      0   \n",
       "4                                     OMG IM CRYINGï»¿      0   \n",
       "\n",
       "                                               title   views  dislikes  \\\n",
       "0  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "1  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "2  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "3  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "4  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "\n",
       "   commentCount  likes  replies           id  \\\n",
       "0          1123  19392        0  mak_Cu9Wl6w   \n",
       "1          1123  19392        0  mak_Cu9Wl6w   \n",
       "2          1123  19392        0  mak_Cu9Wl6w   \n",
       "3          1123  19392        0  mak_Cu9Wl6w   \n",
       "4          1123  19392        0  mak_Cu9Wl6w   \n",
       "\n",
       "                                               clean  \n",
       "0  What lucki guy, got celebr birthday concert bi...  \n",
       "1                                           Love itï»¿  \n",
       "2          american even knew corden sever year agoï»¿  \n",
       "3               birthday 22nd well support west hamï»¿  \n",
       "4                                     OMG IM CRYINGï»¿  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BadWordCounter(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        with open(\"my_badlist3.txt\") as f:\n",
    "            badwords = [l.strip() for l in f.readlines()]\n",
    "        self.badwords_ = badwords\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return np.array(['n_words', 'n_chars', 'allcaps', 'max_len',\n",
    "            'mean_len', '@', '!', 'spaces', 'bad_ratio', 'n_bad',\n",
    "            'capsratio'])\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents): \n",
    "        ## some handcrafted features!\n",
    "        n_words = [len(c.split()) for c in documents]\n",
    "        n_chars = [len(c) for c in documents]\n",
    "        # number of uppercase words\n",
    "        allcaps = [np.sum([w.isupper() for w in c.split()])\n",
    "               for c in documents]\n",
    "        # longest word\n",
    "        max_word_len = [np.max([len(w) for w in c.split()]) for c in (documents)]\n",
    "        # average word length\n",
    "        mean_word_len = [np.mean([len(w) for w in c.split()])\n",
    "                                            for c in (documents)]\n",
    "        # number of google badwords:\n",
    "        n_bad = [np.sum([c.lower().count(w) for w in self.badwords_])\n",
    "                                                for c in documents]\n",
    "        exclamation = [c.count(\"!\") for c in documents]\n",
    "        addressing = [c.count(\"@\") for c in documents]\n",
    "        spaces = [c.count(\" \") for c in documents]\n",
    "\n",
    "        n_words[n_words==0] = 1\n",
    "        allcaps_ratio = np.array(allcaps) / np.array(n_words, dtype=np.float)\n",
    "        bad_ratio = np.array(n_bad) / np.array(n_words, dtype=np.float)\n",
    "        \n",
    "        output = np.array([n_words, n_chars, allcaps, max_word_len,\n",
    "            mean_word_len, exclamation, addressing, spaces, bad_ratio, n_bad,\n",
    "            allcaps_ratio]).T\n",
    "\n",
    "        return normalize(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureStacker(BaseEstimator):\n",
    "    \"\"\"Stacks several transformer objects to yield concatenated features.\n",
    "    Similar to pipeline, a list of tuples ``(name, estimator)`` is passed\n",
    "    to the constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer_list):\n",
    "        self.transformer_list = transformer_list\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for name, trans in self.transformer_list:\n",
    "            trans.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for name, trans in self.transformer_list:\n",
    "            features.append(trans.transform(X))\n",
    "        issparse = [sparse.issparse(f) for f in features]\n",
    "        if np.any(issparse):\n",
    "            features = sparse.hstack(features).tocsr()\n",
    "        else:\n",
    "            features = np.hstack(features)\n",
    "        return features\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        if not deep:\n",
    "            return super(FeatureStacker, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = dict(self.transformer_list)\n",
    "            for name, trans in self.transformer_list:\n",
    "                for key, value in trans.get_params(deep=True).iteritems():\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_word = TfidfVectorizer(lowercase=False,\n",
    "                             analyzer=u'word',\n",
    "                             ngram_range=(1, 3),\n",
    "                             stop_words='english',\n",
    "                             binary=False,\n",
    "                             norm=u'l2', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True, \n",
    "                             sublinear_tf=True,\n",
    "                             min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_char = TfidfVectorizer(lowercase=False,\n",
    "                             analyzer=u'char',\n",
    "                             ngram_range=(1, 5),\n",
    "                             stop_words='english',\n",
    "                             binary=False,\n",
    "                             norm=u'l2', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True, \n",
    "                             sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select = SelectPercentile(score_func=chi2, percentile=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=1e-8, penalty='l2', C=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "badwords = BadWordCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ft = FeatureStacker([(\"badwords\", badwords), (\"chars\", vectorizer_char), (\"words\", vectorizer_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', ft), ('select', select), ('logr', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(data.troll.values)\n",
    "Y = le.transform(data.troll.values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.clean, Y, train_size=.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', FeatureStacker(transformer_list=[('badwords', BadWordCounter()), ('chars', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       " ...lty='l2', random_state=None, solver='liblinear', tol=1e-08,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence matrix:\n",
      "[[689  14]\n",
      " [ 56  28]]\n"
     ]
    }
   ],
   "source": [
    "score = confusion_matrix(y_test, pred)\n",
    "    \n",
    "print('confidence matrix:')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives:\n",
      "================\n",
      "17      This clown is like shit on a field. Unfunny fa...\n",
      "57                                         fecking awfulï»¿\n",
      "84                                 What a bunch of cuntsï»¿\n",
      "229     6 months? ...fuck that. At least give people a...\n",
      "307                                        cheating scumï»¿\n",
      "308                                               niggerï»¿\n",
      "716     if he laughed in front of me I&#39;d be fuckin...\n",
      "744                                 what the actual fuckï»¿\n",
      "758     Fuck this website the like ratio shows how int...\n",
      "777                                           goofy fuckï»¿\n",
      "778           Why are you saying that your face is grossï»¿\n",
      "785                                          Wtf is thisï»¿\n",
      "852                                               suckedï»¿\n",
      "893     Jumping off a boat into WATER is news these da...\n",
      "895     I hope that bitch drowns, that black lives mat...\n",
      "901                     Why is this a fucking news storyï»¿\n",
      "902                                       TMZ is so shitï»¿\n",
      "910                                         Fuck you TZMï»¿\n",
      "914                          Man she&#39;s ugly as fuck.ï»¿\n",
      "916                                             fuck tmzï»¿\n",
      "920                                     who gives a fuckï»¿\n",
      "921     TMZ full of Cringey ass white people who arent...\n",
      "925     WHO THE FUCK CARES THIS SHIT IS TRENDING AND I...\n",
      "927        Tmz, the type of niggas to put in fake laughsï»¿\n",
      "935                           everyone at t mz are cuntsï»¿\n",
      "936                           Seriously, Fuck that cunt.ï»¿\n",
      "940     this is news to you?  WHO THE FUCK CARES FOR T...\n",
      "942     Imagine being stuck in a room with all those c...\n",
      "944                             tmz are sycophantic scumï»¿\n",
      "951     wtf is this, why is this news... tmz is n cancerï»¿\n",
      "                              ...                        \n",
      "3386                                              retardï»¿\n",
      "3390                              yer off ya fuckin headï»¿\n",
      "3396                                         Bullshit!!!ï»¿\n",
      "3400                              Trash video , Faggot !ï»¿\n",
      "3413                                         Sad prick ðŸ˜‚ï»¿\n",
      "3418                                       Go die stupidï»¿\n",
      "3425            hw was juiced up to his fucking eyeballsï»¿\n",
      "3427                                 Fucking Neanderthalï»¿\n",
      "3429    This fucking juice head has no place in the UF...\n",
      "3433                                     Fuck that pussyï»¿\n",
      "3447                                 Fuck ugly sted headï»¿\n",
      "3449                                 Cheating lying cuntï»¿\n",
      "3451                                Definitely a cunt...ï»¿\n",
      "3456    OF COURSE THESE RETARDED WWE GUYS DONT KNOW WH...\n",
      "3458    Fuck off,your last fight was fucking awful,cir...\n",
      "3464                   Fuck off you mad fuckin juiceheadï»¿\n",
      "3471                                    Cheating scumbagï»¿\n",
      "3475         Stay the fuck out of mma you cheating fuck.ï»¿\n",
      "3488                  put more roids in your ass cheaterï»¿\n",
      "3493    Stay the fuck out of my sport cheating piece o...\n",
      "3495             Fuck off you stupid fucking doping shitï»¿\n",
      "3496    Go fuck yourself u jucing little excuse using ...\n",
      "3500    Fuck NO Lesnar UFC fight Fans Dont want you <b...\n",
      "3515                                   PED peace of shitï»¿\n",
      "3518                                        Ha ha doucheï»¿\n",
      "3723                           What a cunt at the end...ï»¿\n",
      "3736                               Surprise dumb fucker!ï»¿\n",
      "3746    fuck Connor and fuck Connor&#39;s fans bunch o...\n",
      "3749           Gilbert is a fucking bum lol. so annoyingï»¿\n",
      "3784                      conors fucken lame. and scaredï»¿\n",
      "Name: comment, dtype: object\n",
      "False Positives:\n",
      "================\n",
      "197      i have Celiac&#39;s disease, this fucking sucksï»¿\n",
      "227     Is it me or am I the only one who thinks Cynth...\n",
      "275                                  People are assholesï»¿\n",
      "354                            whooooooooopie fucking doï»¿\n",
      "435                                                  wtfï»¿\n",
      "693                                     Cured my cancer.ï»¿\n",
      "766                                                  wtfï»¿\n",
      "1159                      I fucked 1,000 bitches - Lil Bï»¿\n",
      "1166                                  stank pussy ass hoï»¿\n",
      "1186    now I slandered a couple of heads on Superhead...\n",
      "1491     Feminism videos are cancer cancer cancer cancerï»¿\n",
      "1544                            This is funny as fuckðŸ˜‚ðŸ˜‚ðŸ˜‚ï»¿\n",
      "2574                      ahahah Scots ate fucking funnyï»¿\n",
      "2640                               what the fuck is thatï»¿\n",
      "2660                                     Fucking boggingï»¿\n",
      "2662    &quot;Why is it so fucking green&quot; &quot;i...\n",
      "2829                                                 Wtfï»¿\n",
      "3066                           our ozone layer is fuckedï»¿\n",
      "3118    iPhone 7 will fuck the shit out of that lagdro...\n",
      "3201    Filmed sideways like a dumb retard from the Un...\n",
      "3268                 don&#39;t fuck with my dick bitchðŸ˜‚ðŸ˜‚ï»¿\n",
      "3443    &#39;ll get through it, you haters go fuck you...\n",
      "3492                      Roided to the fucking eyeballsï»¿\n",
      "Name: comment, dtype: object\n",
      "False Negatives:\n",
      "================\n",
      "13      wot a surprise can&#39;t get away from his cub...\n",
      "94           James cordens stylist ðŸ˜‚ fucking state of itï»¿\n",
      "236       ehhh fine...i&#39;ll give ya a like..assholes.ï»¿\n",
      "257     Don&#39;t agree with the gluten free one. Fuck...\n",
      "259                  kys college humor you&#39;re cancerï»¿\n",
      "322                            FUCK YOU CHINA WE GOT 2NDï»¿\n",
      "331       was that good for a blackman chucking a spear?ï»¿\n",
      "339     this nigga was the cause of my dad getting &qu...\n",
      "380                                   Well that was shitï»¿\n",
      "383     how the fuck was that even amazing? he threw i...\n",
      "464              Another shitty female lead action movieï»¿\n",
      "466     Dafaq is this shit? Its exactly the same as th...\n",
      "467     I don&#39;t think I can survive two hours of H...\n",
      "592     Why is this kind of stuff on YouTube. It&#39;s...\n",
      "703                                               Cancerï»¿\n",
      "735     ..... I can&#39;t decide if that funny or sad....\n",
      "738                                    how is this funnyï»¿\n",
      "740                                     waste of my timeï»¿\n",
      "750     U shouldn&#39;t go around calling people&#39;s...\n",
      "770     This shit was trending when it had 2k views. S...\n",
      "772     how is this on trending cause that is what mos...\n",
      "774     R.I.P to that 20seconds Ill never get back.  I...\n",
      "831                           dafuck is this. no just noï»¿\n",
      "832     Miley completely and utterly destroyed a classicï»¿\n",
      "836     They all suck they should take notes from Jaco...\n",
      "891                                   I wish she drownedï»¿\n",
      "894                         30 second ad!!! Wtf is this?ï»¿\n",
      "898     I would love to ram the big man right up her h...\n",
      "899     It&#39;s a shame this racist bitch came back u...\n",
      "903     imagine having these cunts follow you around t...\n",
      "                              ...                        \n",
      "3077                 it stunk years ago and still stinksï»¿\n",
      "3103    Steve Job was an asshoe and only asshoes buy a...\n",
      "3357                                     Bull. Shit. lolï»¿\n",
      "3365    Jesus fucking Christ you really need help. als...\n",
      "3373    disliked before watching you fucking illuminat...\n",
      "3374    The only evil in the world is created by fucki...\n",
      "3408    lol this guy can&#39;t be real, he is beyond s...\n",
      "3411    Who else clicked this just to laugh at how stu...\n",
      "3428                     His head is like a bowling ballï»¿\n",
      "3444    Fucking hell this guy looks like a bloody potatoï»¿\n",
      "3452                          guy looks like a zika babyï»¿\n",
      "3460          I&#39;d fuck this shaved albino gorilla upï»¿\n",
      "3466               I take shits bigger than Brock Lesnarï»¿\n",
      "3485                                Steroid using cowardï»¿\n",
      "3490    NATE DIAZ HIT THE DECK 3 TIMES LIKE A BITCH. Q...\n",
      "3503    Roid head.. Im not surprise if this dude die o...\n",
      "3504    PATHETIC BASEMENT DWELLING VIRGINS DEFENDING H...\n",
      "3506                                             cheaterï»¿\n",
      "3509    THESE FAGGOTS ARE COMMENTING SAYING HE WOULD B...\n",
      "3511    COMPLETE DISGUSTING JUICED UP BITCH, ALL HIS F...\n",
      "3516                                     Complete doucheï»¿\n",
      "3565                                       fuck ya Eddieï»¿\n",
      "3599              What dickhead give this a thumbs down?ï»¿\n",
      "3724                           What a bellend maguire isï»¿\n",
      "3732    Conor: Fuck Diaz&#39;s team... and if you don&...\n",
      "3737                        Suprise suprise motherfuckerï»¿\n",
      "3810                              Fuck the Diaz brothersï»¿\n",
      "3825    Conor is such a little bitch. He&#39;s going t...\n",
      "3829    I hate this guy. Hes a huge mistake!Just like ...\n",
      "3912                                     Trudeau=A$$H0LEï»¿\n",
      "Name: comment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "clf_labels = model.predict(data.clean)\n",
    "print('True Positives:')\n",
    "print('================')\n",
    "print(data['comment'][(clf_labels==1) & (data.troll==1)])\n",
    "print('False Positives:')\n",
    "print('================')\n",
    "print(data['comment'][(clf_labels==1) & (data.troll==0)])\n",
    "print('False Negatives:')\n",
    "print('================')\n",
    "print(data['comment'][(clf_labels==0) & (data.troll==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_steps['select'].get_support().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "* does it make sense to do variance based feature selection when dealing with imbalanced label sizes?\n",
    "* tf-idf vectorizer gives around 5500 words (columns) and that's more than the number of rows, and since most words are redundant anyway it is important to reduce the dimensionality of the problem\n",
    "* the fact that only 10% of the data is trolls means that maybe an anamoly detection approach would perform better, this is taken care of by giving the classifier the flag class_weight='balanced' that ensures a balanced sampling of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
