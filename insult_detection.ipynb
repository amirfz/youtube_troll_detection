{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize, LabelEncoder\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('youtube-comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>troll</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a lucky guy, got celebrated birthday on c...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love it﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no americans even knew who corden was several ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my birthday was the 22nd as well and we both s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMG IM CRYING﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  troll  \\\n",
       "0  What a lucky guy, got celebrated birthday on c...      0   \n",
       "1                                           Love it﻿      0   \n",
       "2  no americans even knew who corden was several ...      0   \n",
       "3  my birthday was the 22nd as well and we both s...      0   \n",
       "4                                     OMG IM CRYING﻿      0   \n",
       "\n",
       "                                               title   views  dislikes  \\\n",
       "0  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "1  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "2  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "3  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "4  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "\n",
       "   commentCount  likes  replies           id  \n",
       "0          1123  19392        0  mak_Cu9Wl6w  \n",
       "1          1123  19392        0  mak_Cu9Wl6w  \n",
       "2          1123  19392        0  mak_Cu9Wl6w  \n",
       "3          1123  19392        0  mak_Cu9Wl6w  \n",
       "4          1123  19392        0  mak_Cu9Wl6w  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    '''Function to clean the text data and prep for further analysis'''\n",
    "    stops = set(stopwords.words(\"english\"))     # Creating a set of Stopwords\n",
    "    p_stemmer = PorterStemmer()                 # Creating the stemmer model\n",
    "    text = re.sub(\"&#39;\",'', text)\n",
    "    text = re.sub(r\"</?\\w+[^>]*>\", ' tag ', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    text = re.sub(\"[^a-zA-Z@!0-9]\", ' ', text)\n",
    "    text = text.split()                          # Splits the data into individual words \n",
    "    text = [w for w in text if not w in stops]   # Removes stopwords\n",
    "    text = [p_stemmer.stem(i) for i in text]     # Stemming (reducing words to their root)\n",
    "    if not len(text):                            # dealing with comments that are all emojis, stop words or other languages\n",
    "        text = ['emostwol']\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['clean'] = data['comment'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>troll</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>id</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a lucky guy, got celebrated birthday on c...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>What lucki guy, got celebr birthday concert bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love it﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>Love it﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no americans even knew who corden was several ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>american even knew corden sever year ago﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my birthday was the 22nd as well and we both s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>birthday 22nd well support west ham﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OMG IM CRYING﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>Nothing Compares 2 U (Live in LA w/ James Corden)</td>\n",
       "      <td>469414</td>\n",
       "      <td>263</td>\n",
       "      <td>1123</td>\n",
       "      <td>19392</td>\n",
       "      <td>0</td>\n",
       "      <td>mak_Cu9Wl6w</td>\n",
       "      <td>OMG IM CRYING﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  troll  \\\n",
       "0  What a lucky guy, got celebrated birthday on c...      0   \n",
       "1                                           Love it﻿      0   \n",
       "2  no americans even knew who corden was several ...      0   \n",
       "3  my birthday was the 22nd as well and we both s...      0   \n",
       "4                                     OMG IM CRYING﻿      0   \n",
       "\n",
       "                                               title   views  dislikes  \\\n",
       "0  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "1  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "2  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "3  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "4  Nothing Compares 2 U (Live in LA w/ James Corden)  469414       263   \n",
       "\n",
       "   commentCount  likes  replies           id  \\\n",
       "0          1123  19392        0  mak_Cu9Wl6w   \n",
       "1          1123  19392        0  mak_Cu9Wl6w   \n",
       "2          1123  19392        0  mak_Cu9Wl6w   \n",
       "3          1123  19392        0  mak_Cu9Wl6w   \n",
       "4          1123  19392        0  mak_Cu9Wl6w   \n",
       "\n",
       "                                               clean  \n",
       "0  What lucki guy, got celebr birthday concert bi...  \n",
       "1                                           Love it﻿  \n",
       "2          american even knew corden sever year ago﻿  \n",
       "3               birthday 22nd well support west ham﻿  \n",
       "4                                     OMG IM CRYING﻿  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BadWordCounter(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        with open(\"my_badlist3.txt\") as f:\n",
    "            badwords = [l.strip() for l in f.readlines()]\n",
    "        self.badwords_ = badwords\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return np.array(['n_words', 'n_chars', 'allcaps', 'max_len',\n",
    "            'mean_len', '@', '!', 'spaces', 'bad_ratio', 'n_bad',\n",
    "            'capsratio'])\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents): \n",
    "        ## some handcrafted features!\n",
    "        n_words = [len(c.split()) for c in documents]\n",
    "        n_chars = [len(c) for c in documents]\n",
    "        # number of uppercase words\n",
    "        allcaps = [np.sum([w.isupper() for w in c.split()])\n",
    "               for c in documents]\n",
    "        # longest word\n",
    "        max_word_len = [np.max([len(w) for w in c.split()]) for c in (documents)]\n",
    "        # average word length\n",
    "        mean_word_len = [np.mean([len(w) for w in c.split()])\n",
    "                                            for c in (documents)]\n",
    "        # number of google badwords:\n",
    "        n_bad = [np.sum([c.lower().count(w) for w in self.badwords_])\n",
    "                                                for c in documents]\n",
    "        exclamation = [c.count(\"!\") for c in documents]\n",
    "        addressing = [c.count(\"@\") for c in documents]\n",
    "        spaces = [c.count(\" \") for c in documents]\n",
    "\n",
    "        n_words[n_words==0] = 1\n",
    "        allcaps_ratio = np.array(allcaps) / np.array(n_words, dtype=np.float)\n",
    "        bad_ratio = np.array(n_bad) / np.array(n_words, dtype=np.float)\n",
    "        \n",
    "        output = np.array([n_words, n_chars, allcaps, max_word_len,\n",
    "            mean_word_len, exclamation, addressing, spaces, bad_ratio, n_bad,\n",
    "            allcaps_ratio]).T\n",
    "\n",
    "        return normalize(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureStacker(BaseEstimator):\n",
    "    \"\"\"Stacks several transformer objects to yield concatenated features.\n",
    "    Similar to pipeline, a list of tuples ``(name, estimator)`` is passed\n",
    "    to the constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer_list):\n",
    "        self.transformer_list = transformer_list\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for name, trans in self.transformer_list:\n",
    "            trans.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for name, trans in self.transformer_list:\n",
    "            features.append(trans.transform(X))\n",
    "        issparse = [sparse.issparse(f) for f in features]\n",
    "        if np.any(issparse):\n",
    "            features = sparse.hstack(features).tocsr()\n",
    "        else:\n",
    "            features = np.hstack(features)\n",
    "        return features\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        if not deep:\n",
    "            return super(FeatureStacker, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = dict(self.transformer_list)\n",
    "            for name, trans in self.transformer_list:\n",
    "                for key, value in trans.get_params(deep=True).iteritems():\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_word = TfidfVectorizer(lowercase=False,\n",
    "                             analyzer=u'word',\n",
    "                             ngram_range=(1, 3),\n",
    "                             stop_words='english',\n",
    "                             binary=False,\n",
    "                             norm=u'l2', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True, \n",
    "                             sublinear_tf=True,\n",
    "                             min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_char = TfidfVectorizer(lowercase=False,\n",
    "                             analyzer=u'char',\n",
    "                             ngram_range=(1, 5),\n",
    "                             stop_words='english',\n",
    "                             binary=False,\n",
    "                             norm=u'l2', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True, \n",
    "                             sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select = SelectPercentile(score_func=chi2, percentile=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=1e-8, penalty='l2', C=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "badwords = BadWordCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ft = FeatureStacker([(\"badwords\", badwords), (\"chars\", vectorizer_char), (\"words\", vectorizer_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Pipeline([('vect', ft), ('select', select), ('logr', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(data.troll.values)\n",
    "Y = le.transform(data.troll.values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.clean, Y, train_size=.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', FeatureStacker(transformer_list=[('badwords', BadWordCounter()), ('chars', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       " ...lty='l2', random_state=None, solver='liblinear', tol=1e-08,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence matrix:\n",
      "[[689  14]\n",
      " [ 56  28]]\n"
     ]
    }
   ],
   "source": [
    "score = confusion_matrix(y_test, pred)\n",
    "    \n",
    "print('confidence matrix:')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives:\n",
      "================\n",
      "17      This clown is like shit on a field. Unfunny fa...\n",
      "57                                         fecking awful﻿\n",
      "84                                 What a bunch of cunts﻿\n",
      "229     6 months? ...fuck that. At least give people a...\n",
      "307                                        cheating scum﻿\n",
      "308                                               nigger﻿\n",
      "716     if he laughed in front of me I&#39;d be fuckin...\n",
      "744                                 what the actual fuck﻿\n",
      "758     Fuck this website the like ratio shows how int...\n",
      "777                                           goofy fuck﻿\n",
      "778           Why are you saying that your face is gross﻿\n",
      "785                                          Wtf is this﻿\n",
      "852                                               sucked﻿\n",
      "893     Jumping off a boat into WATER is news these da...\n",
      "895     I hope that bitch drowns, that black lives mat...\n",
      "901                     Why is this a fucking news story﻿\n",
      "902                                       TMZ is so shit﻿\n",
      "910                                         Fuck you TZM﻿\n",
      "914                          Man she&#39;s ugly as fuck.﻿\n",
      "916                                             fuck tmz﻿\n",
      "920                                     who gives a fuck﻿\n",
      "921     TMZ full of Cringey ass white people who arent...\n",
      "925     WHO THE FUCK CARES THIS SHIT IS TRENDING AND I...\n",
      "927        Tmz, the type of niggas to put in fake laughs﻿\n",
      "935                           everyone at t mz are cunts﻿\n",
      "936                           Seriously, Fuck that cunt.﻿\n",
      "940     this is news to you?  WHO THE FUCK CARES FOR T...\n",
      "942     Imagine being stuck in a room with all those c...\n",
      "944                             tmz are sycophantic scum﻿\n",
      "951     wtf is this, why is this news... tmz is n cancer﻿\n",
      "                              ...                        \n",
      "3386                                              retard﻿\n",
      "3390                              yer off ya fuckin head﻿\n",
      "3396                                         Bullshit!!!﻿\n",
      "3400                              Trash video , Faggot !﻿\n",
      "3413                                         Sad prick 😂﻿\n",
      "3418                                       Go die stupid﻿\n",
      "3425            hw was juiced up to his fucking eyeballs﻿\n",
      "3427                                 Fucking Neanderthal﻿\n",
      "3429    This fucking juice head has no place in the UF...\n",
      "3433                                     Fuck that pussy﻿\n",
      "3447                                 Fuck ugly sted head﻿\n",
      "3449                                 Cheating lying cunt﻿\n",
      "3451                                Definitely a cunt...﻿\n",
      "3456    OF COURSE THESE RETARDED WWE GUYS DONT KNOW WH...\n",
      "3458    Fuck off,your last fight was fucking awful,cir...\n",
      "3464                   Fuck off you mad fuckin juicehead﻿\n",
      "3471                                    Cheating scumbag﻿\n",
      "3475         Stay the fuck out of mma you cheating fuck.﻿\n",
      "3488                  put more roids in your ass cheater﻿\n",
      "3493    Stay the fuck out of my sport cheating piece o...\n",
      "3495             Fuck off you stupid fucking doping shit﻿\n",
      "3496    Go fuck yourself u jucing little excuse using ...\n",
      "3500    Fuck NO Lesnar UFC fight Fans Dont want you <b...\n",
      "3515                                   PED peace of shit﻿\n",
      "3518                                        Ha ha douche﻿\n",
      "3723                           What a cunt at the end...﻿\n",
      "3736                               Surprise dumb fucker!﻿\n",
      "3746    fuck Connor and fuck Connor&#39;s fans bunch o...\n",
      "3749           Gilbert is a fucking bum lol. so annoying﻿\n",
      "3784                      conors fucken lame. and scared﻿\n",
      "Name: comment, dtype: object\n",
      "False Positives:\n",
      "================\n",
      "197      i have Celiac&#39;s disease, this fucking sucks﻿\n",
      "227     Is it me or am I the only one who thinks Cynth...\n",
      "275                                  People are assholes﻿\n",
      "354                            whooooooooopie fucking do﻿\n",
      "435                                                  wtf﻿\n",
      "693                                     Cured my cancer.﻿\n",
      "766                                                  wtf﻿\n",
      "1159                      I fucked 1,000 bitches - Lil B﻿\n",
      "1166                                  stank pussy ass ho﻿\n",
      "1186    now I slandered a couple of heads on Superhead...\n",
      "1491     Feminism videos are cancer cancer cancer cancer﻿\n",
      "1544                            This is funny as fuck😂😂😂﻿\n",
      "2574                      ahahah Scots ate fucking funny﻿\n",
      "2640                               what the fuck is that﻿\n",
      "2660                                     Fucking bogging﻿\n",
      "2662    &quot;Why is it so fucking green&quot; &quot;i...\n",
      "2829                                                 Wtf﻿\n",
      "3066                           our ozone layer is fucked﻿\n",
      "3118    iPhone 7 will fuck the shit out of that lagdro...\n",
      "3201    Filmed sideways like a dumb retard from the Un...\n",
      "3268                 don&#39;t fuck with my dick bitch😂😂﻿\n",
      "3443    &#39;ll get through it, you haters go fuck you...\n",
      "3492                      Roided to the fucking eyeballs﻿\n",
      "Name: comment, dtype: object\n",
      "False Negatives:\n",
      "================\n",
      "13      wot a surprise can&#39;t get away from his cub...\n",
      "94           James cordens stylist 😂 fucking state of it﻿\n",
      "236       ehhh fine...i&#39;ll give ya a like..assholes.﻿\n",
      "257     Don&#39;t agree with the gluten free one. Fuck...\n",
      "259                  kys college humor you&#39;re cancer﻿\n",
      "322                            FUCK YOU CHINA WE GOT 2ND﻿\n",
      "331       was that good for a blackman chucking a spear?﻿\n",
      "339     this nigga was the cause of my dad getting &qu...\n",
      "380                                   Well that was shit﻿\n",
      "383     how the fuck was that even amazing? he threw i...\n",
      "464              Another shitty female lead action movie﻿\n",
      "466     Dafaq is this shit? Its exactly the same as th...\n",
      "467     I don&#39;t think I can survive two hours of H...\n",
      "592     Why is this kind of stuff on YouTube. It&#39;s...\n",
      "703                                               Cancer﻿\n",
      "735     ..... I can&#39;t decide if that funny or sad....\n",
      "738                                    how is this funny﻿\n",
      "740                                     waste of my time﻿\n",
      "750     U shouldn&#39;t go around calling people&#39;s...\n",
      "770     This shit was trending when it had 2k views. S...\n",
      "772     how is this on trending cause that is what mos...\n",
      "774     R.I.P to that 20seconds Ill never get back.  I...\n",
      "831                           dafuck is this. no just no﻿\n",
      "832     Miley completely and utterly destroyed a classic﻿\n",
      "836     They all suck they should take notes from Jaco...\n",
      "891                                   I wish she drowned﻿\n",
      "894                         30 second ad!!! Wtf is this?﻿\n",
      "898     I would love to ram the big man right up her h...\n",
      "899     It&#39;s a shame this racist bitch came back u...\n",
      "903     imagine having these cunts follow you around t...\n",
      "                              ...                        \n",
      "3077                 it stunk years ago and still stinks﻿\n",
      "3103    Steve Job was an asshoe and only asshoes buy a...\n",
      "3357                                     Bull. Shit. lol﻿\n",
      "3365    Jesus fucking Christ you really need help. als...\n",
      "3373    disliked before watching you fucking illuminat...\n",
      "3374    The only evil in the world is created by fucki...\n",
      "3408    lol this guy can&#39;t be real, he is beyond s...\n",
      "3411    Who else clicked this just to laugh at how stu...\n",
      "3428                     His head is like a bowling ball﻿\n",
      "3444    Fucking hell this guy looks like a bloody potato﻿\n",
      "3452                          guy looks like a zika baby﻿\n",
      "3460          I&#39;d fuck this shaved albino gorilla up﻿\n",
      "3466               I take shits bigger than Brock Lesnar﻿\n",
      "3485                                Steroid using coward﻿\n",
      "3490    NATE DIAZ HIT THE DECK 3 TIMES LIKE A BITCH. Q...\n",
      "3503    Roid head.. Im not surprise if this dude die o...\n",
      "3504    PATHETIC BASEMENT DWELLING VIRGINS DEFENDING H...\n",
      "3506                                             cheater﻿\n",
      "3509    THESE FAGGOTS ARE COMMENTING SAYING HE WOULD B...\n",
      "3511    COMPLETE DISGUSTING JUICED UP BITCH, ALL HIS F...\n",
      "3516                                     Complete douche﻿\n",
      "3565                                       fuck ya Eddie﻿\n",
      "3599              What dickhead give this a thumbs down?﻿\n",
      "3724                           What a bellend maguire is﻿\n",
      "3732    Conor: Fuck Diaz&#39;s team... and if you don&...\n",
      "3737                        Suprise suprise motherfucker﻿\n",
      "3810                              Fuck the Diaz brothers﻿\n",
      "3825    Conor is such a little bitch. He&#39;s going t...\n",
      "3829    I hate this guy. Hes a huge mistake!Just like ...\n",
      "3912                                     Trudeau=A$$H0LE﻿\n",
      "Name: comment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "clf_labels = model.predict(data.clean)\n",
    "print('True Positives:')\n",
    "print('================')\n",
    "print(data['comment'][(clf_labels==1) & (data.troll==1)])\n",
    "print('False Positives:')\n",
    "print('================')\n",
    "print(data['comment'][(clf_labels==1) & (data.troll==0)])\n",
    "print('False Negatives:')\n",
    "print('================')\n",
    "print(data['comment'][(clf_labels==0) & (data.troll==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_steps['select'].get_support().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "* does it make sense to do variance based feature selection when dealing with imbalanced label sizes?\n",
    "* tf-idf vectorizer gives around 5500 words (columns) and that's more than the number of rows, and since most words are redundant anyway it is important to reduce the dimensionality of the problem\n",
    "* the fact that only 10% of the data is trolls means that maybe an anamoly detection approach would perform better, this is taken care of by giving the classifier the flag class_weight='balanced' that ensures a balanced sampling of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
